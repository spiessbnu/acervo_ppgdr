import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import ast
import unicodedata
import re
import numpy as np
import nltk
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from nltk.stem import RSLPStemmer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import networkx as nx
import pickle
import textwrap
import openai
from openai import OpenAI

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Acervo PPGDR v1",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personalizado para o design branco com destaques azuis
st.markdown("""
<style>
    .main {
        background-color: white;
    }
    
    .stButton > button {
        background-color: #0F5EDD;
        color: white;
        border: none;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        font-weight: 500;
    }
    
    .stButton > button:hover {
        background-color: #0A4BC7;
        color: white;
    }
    
    .stSelectbox > div > div {
        background-color: white;
        border: 1px solid #0F5EDD;
    }
    
    .stTextInput > div > div > input {
        border: 1px solid #0F5EDD;
    }
    
    .metric-card {
        background-color: #F8FAFF;
        border: 1px solid #0F5EDD;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
    }
    
    .header-title {
        color: #0F5EDD;
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .section-header {
        color: #0F5EDD;
        font-size: 1.5rem;
        font-weight: bold;
        margin: 1rem 0;
        border-bottom: 2px solid #0F5EDD;
        padding-bottom: 0.5rem;
    }
    
    .expandable-content {
        background-color: #F8FAFF;
        border-left: 4px solid #0F5EDD;
        padding: 1rem;
        margin: 1rem 0;
        border-radius: 0 5px 5px 0;
    }
    
    .similarity-score {
        background-color: #0F5EDD;
        color: white;
        padding: 0.2rem 0.5rem;
        border-radius: 3px;
        font-size: 0.8rem;
        font-weight: bold;
    }
    
    .embedding-badge {
        background-color: #28a745;
        color: white;
        padding: 0.2rem 0.5rem;
        border-radius: 3px;
        font-size: 0.7rem;
        font-weight: bold;
    }
    
    .api-badge {
        background-color: #17a2b8;
        color: white;
        padding: 0.2rem 0.5rem;
        border-radius: 3px;
        font-size: 0.7rem;
        font-weight: bold;
    }
    
    .summary-box {
        background-color: #F8FAFF;
        border: 2px solid #0F5EDD;
        border-radius: 10px;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    
    .cluster-info {
        background-color: #E3F2FD;
        border-left: 4px solid #0F5EDD;
        padding: 1rem;
        margin: 0.5rem 0;
        border-radius: 0 5px 5px 0;
    }
</style>
""", unsafe_allow_html=True)

# Fun√ß√µes auxiliares
@st.cache_data
def load_data():
    """Carrega os dados do CSV"""
    try:
        df = pd.read_csv('dados_finais_com_resumo_llm.csv')
        return df
    except FileNotFoundError:
        st.error("Arquivo 'dados_finais_com_resumo_llm.csv' n√£o encontrado. Por favor, fa√ßa o upload do arquivo.")
        return pd.DataFrame()

@st.cache_data
def load_embeddings():
    """Carrega os embeddings pr√©-calculados"""
    try:
        # Carrega o arquivo correto de embeddings
        embeddings = np.load('openai_embeddings_concatenado_large.npy')
        st.success(f"‚úÖ Embeddings carregados com sucesso! Dimens√£o: {embeddings.shape}")
        return embeddings
    except FileNotFoundError:
        st.warning("‚ö†Ô∏è Arquivo 'openai_embeddings_concatenado_large.npy' n√£o encontrado. A busca sem√¢ntica avan√ßada n√£o estar√° dispon√≠vel.")
        return None
    except Exception as e:
        st.error(f"‚ùå Erro ao carregar embeddings: {str(e)}")
        return None

@st.cache_resource
def init_openai_client():
    """Inicializa o cliente OpenAI a partir de st.secrets."""
    try:
        key = st.secrets.get("openai_api_key", "").strip()
        if not key:
            st.warning("‚ö†Ô∏è OpenAI API key n√£o configurada em st.secrets.")
            return None
        client = OpenAI(api_key=key)
        # Optional: testar conex√£o ‚Äî mas cuidado com rate limits
        # client.models.list()
        return client
    except Exception as e:
        st.error(f"‚ùå Erro ao inicializar cliente OpenAI: {e}")
        return None

def safe_literal_eval(s):
    """Converte string em lista de forma segura"""
    try:
        return ast.literal_eval(s)
    except (ValueError, SyntaxError, TypeError):
        return []

def remover_acentos(texto):
    """Remove acentos de uma string para ordena√ß√£o"""
    texto_normalizado = unicodedata.normalize('NFD', texto)
    return "".join(c for c in texto_normalizado if not unicodedata.combining(c))

def normalizar_string(texto):
    """Converte para min√∫sculas e remove acentos"""
    if not isinstance(texto, str):
        return ""
    texto_normalizado = unicodedata.normalize('NFD', texto.lower())
    return "".join(c for c in texto_normalizado if not unicodedata.combining(c))

def preprocessar_texto(texto, usar_stemmer=False):
    """Limpa, tokeniza, remove stopwords e opcionalmente aplica stemming"""
    if not isinstance(texto, str):
        return ""
    
    texto_normalizado = normalizar_string(texto)
    texto_limpo = re.sub(r'[^a-zA-Z\s]', '', texto_normalizado)
    tokens = word_tokenize(texto_limpo)
    
    try:
        stopwords_pt = set(stopwords.words('portuguese'))
    except LookupError:
        nltk.download('stopwords')
        stopwords_pt = set(stopwords.words('portuguese'))
    
    tokens_filtrados = [p for p in tokens if p not in stopwords_pt and len(p) > 1]
    
    if usar_stemmer:
        stemmer = RSLPStemmer()
        tokens_finais = [stemmer.stem(p) for p in tokens_filtrados]
    else:
        tokens_finais = tokens_filtrados
    
    return " ".join(tokens_finais)

def buscar_e_rankear(query_texto, usar_stemmer, vectorizer, tfidf_matrix):
    """Processa uma query, calcula a similaridade e retorna um ranking"""
    query_processada = preprocessar_texto(query_texto, usar_stemmer=usar_stemmer)
    if not query_processada:
        return []
    
    query_vetor = vectorizer.transform([query_processada])
    similaridades = cosine_similarity(query_vetor, tfidf_matrix).flatten()
    indices_rankeados = np.argsort(-similaridades)
    
    return [(i, similaridades[i]) for i in indices_rankeados if similaridades[i] > 0.01]

def buscar_semantica_openai_real(query_texto, client, embeddings_documentos, model="text-embedding-3-large"):
    """
    Busca sem√¢ntica real usando API OpenAI para gerar embedding da query.
    """
    if not query_texto.strip() or embeddings_documentos is None or client is None:
        return []
    
    try:
        # Gera o embedding para a query do usu√°rio usando a API OpenAI
        response = client.embeddings.create(
            input=[query_texto], 
            model=model
        )
        query_embedding = response.data[0].embedding
        
        # Calcula a similaridade de cosseno entre o vetor da query e todos os vetores dos documentos
        similaridades = cosine_similarity([query_embedding], embeddings_documentos).flatten()
        
        # Ordena os resultados pela similaridade (do maior para o menor)
        indices_rankeados = np.argsort(-similaridades)
        
        # Retorna o ranking, filtrando resultados com score muito baixo
        return [(i, similaridades[i]) for i in indices_rankeados if similaridades[i] > 0.2]
        
    except Exception as e:
        st.error(f"Erro na busca sem√¢ntica com API OpenAI: {str(e)}")
        return []

def gerar_sumario_cluster(documentos_cluster, client, model="gpt-4o-mini"):
    """
    Gera um sum√°rio tem√°tico para um cluster de documentos usando OpenAI.
    """
    if not documentos_cluster or client is None:
        return "Sum√°rio n√£o dispon√≠vel."
    
    try:
        # Concatena o texto de todos os documentos do cluster
        textos_concatenados = "\n\n--- PR√ìXIMO DOCUMENTO ---\n\n".join(
            [f"T√≠tulo: {doc['T√≠tulo']}\nResumo: {doc['Resumo_LLM']}" for doc in documentos_cluster]
        )
        
        # Prompt para sumariza√ß√£o tem√°tica
        prompt_sistema = """
        Voc√™ √© um assistente de pesquisa acad√™mica altamente qualificado, especializado em identificar e sintetizar os temas centrais de um conjunto de documentos.

        Sua tarefa √© ler todos os textos fornecidos a seguir, que s√£o de trabalhos acad√™micos agrupados por similaridade sem√¢ntica.
        Analise-os e gere um √∫nico par√°grafo coeso que resuma os principais temas, problemas de pesquisa e metodologias abordadas pelo grupo como um todo.
        N√£o descreva cada trabalho individualmente. Foque na ess√™ncia que conecta todos eles.

        Ap√≥s o par√°grafo de s√≠ntese, identifique e liste de 3 a 5 palavras-chave ou conceitos que s√£o os pilares deste agrupamento.

        Formate sua resposta exatamente da seguinte forma:

        **S√≠ntese Tem√°tica:**
        [Seu par√°grafo de resumo aqui]

        **Termos Comuns:**
        - [Termo 1]
        - [Termo 2]
        - [Termo 3]
        """
        
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": prompt_sistema},
                {"role": "user", "content": textos_concatenados}
            ],
            temperature=0.1,
            max_tokens=500,
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        return f"Erro ao gerar sum√°rio: {str(e)}"

def calcular_matriz_similaridade(df, embeddings=None):
    """Calcula matriz de similaridade entre documentos"""
    if embeddings is not None:
        # Usa embeddings se dispon√≠veis
        try:
            matriz_similaridade = cosine_similarity(embeddings)
            return matriz_similaridade
        except Exception as e:
            st.warning(f"Erro ao usar embeddings, usando TF-IDF: {str(e)}")
    
    # Fallback para TF-IDF
    vectorizer, tfidf_matrix, _ = preparar_busca_semantica(df)
    matriz_similaridade = cosine_similarity(tfidf_matrix)
    return matriz_similaridade

        
def criar_rede_similaridade(df, embeddings=None, threshold=0.3, max_nodes=20):
    """Cria visualiza√ß√£o de rede de similaridade usando Plotly"""
    
    # Limita o n√∫mero de documentos para melhor visualiza√ß√£o
    df_sample = df.head(max_nodes).copy()
    
    # Calcula matriz de similaridade
    if embeddings is not None:
        embeddings_sample = embeddings[:max_nodes]
        matriz_sim = cosine_similarity(embeddings_sample)
        st.info(f"üöÄ Usando embeddings reais para calcular similaridade (dimens√£o: {embeddings_sample.shape})")
    else:
        # Usa TF-IDF como fallback
        textos_combinados = []
        for idx, row in df_sample.iterrows():
            texto = str(row['T√≠tulo'])
            if 'Resumo_LLM' in df_sample.columns and pd.notna(row['Resumo_LLM']):
                texto += " " + str(row['Resumo_LLM'])
            textos_combinados.append(texto)
        
        vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))
        tfidf_matrix = vectorizer.fit_transform([preprocessar_texto(texto) for texto in textos_combinados])
        matriz_sim = cosine_similarity(tfidf_matrix)
        st.info("üìä Usando TF-IDF para calcular similaridade")
    
    # Cria grafo
    G = nx.Graph()
    
    # Adiciona n√≥s
    for i, (idx, row) in enumerate(df_sample.iterrows()):
        titulo_curto = row['T√≠tulo'][:50] + "..." if len(row['T√≠tulo']) > 50 else row['T√≠tulo']
        G.add_node(i, 
                  titulo=titulo_curto,
                  titulo_completo=row['T√≠tulo'],
                  autor=row['Autor'],
                  ano=row['Ano'],
                  tipo=row['Tipo_Documento'],
                  resumo=row['Resumo_LLM'] if 'Resumo_LLM' in row else '',
                  idx_original=idx)
    
    # Adiciona arestas baseadas na similaridade
    edges_added = 0
    for i in range(len(df_sample)):
        for j in range(i+1, len(df_sample)):
            similaridade = matriz_sim[i][j]
            if similaridade > threshold:
                G.add_edge(i, j, weight=similaridade)
                edges_added += 1
    
    if edges_added == 0:
        st.warning(f"‚ö†Ô∏è Nenhuma conex√£o encontrada com threshold {threshold}. Tente diminuir o valor.")
        return None, None
    
    # Layout do grafo
    pos = nx.spring_layout(G, k=3, iterations=50, seed=42)
    
    # Prepara dados para Plotly
    edge_x = []
    edge_y = []
    edge_weights = []
    
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.extend([x0, x1, None])
        edge_y.extend([y0, y1, None])
        weight = G[edge[0]][edge[1]]['weight']
        edge_weights.append(weight)
    
    # Cria trace das arestas com espessura baseada no peso
    edge_trace = go.Scatter(
        x=edge_x, y=edge_y,
        line=dict(width=2, color='rgba(15, 94, 221, 0.3)'),
        hoverinfo='none',
        mode='lines'
    )
    
    # Prepara dados dos n√≥s
    node_x = []
    node_y = []
    node_text = []
    node_info = []
    node_color = []
    node_size = []
    
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        
        # Informa√ß√µes do n√≥
        node_data = G.nodes[node]
        node_text.append(f"{node_data['titulo']}<br>({node_data['ano']})")
        
        # Calcular conex√µes e similaridade m√©dia
        neighbors = list(G.neighbors(node))
        num_connections = len(neighbors)
        avg_similarity = np.mean([G[node][neighbor]['weight'] for neighbor in neighbors]) if neighbors else 0
        
        node_info.append(
            f"<b>{node_data['titulo_completo']}</b><br>"
            f"Autor: {node_data['autor']}<br>"
            f"Ano: {node_data['ano']}<br>"
            f"Tipo: {node_data['tipo']}<br>"
            f"Conex√µes: {num_connections}<br>"
            f"Similaridade m√©dia: {avg_similarity:.3f}"
        )
        
        # Cor baseada no tipo de documento
        if node_data['tipo'] == 'Tese':
            node_color.append('#0A4BC7')
        else:
            node_color.append('#0F5EDD')
        
        # Tamanho baseado no n√∫mero de conex√µes
        node_size.append(max(15, 10 + num_connections * 3))
    
    # Cria trace dos n√≥s
    node_trace = go.Scatter(
        x=node_x, y=node_y,
        mode='markers+text',
        hoverinfo='text',
        hovertext=node_info,
        text=[t.split('<br>')[0] for t in node_text],  # S√≥ o t√≠tulo para o texto
        textposition="middle center",
        textfont=dict(size=8, color='white'),
        marker=dict(
            size=node_size,
            color=node_color,
            line=dict(width=2, color='white'),
            opacity=0.9
        )
    )
    
    # Cria figura
    fig = go.Figure(
        data=[edge_trace, node_trace],
        layout=go.Layout(
            title=dict(
                text=f'Rede de Similaridade - {len(df_sample)} Trabalhos ({edges_added} conex√µes)',
                x=0.5,
                font=dict(size=16, color='#0F5EDD')
            ),
            showlegend=False,
            hovermode='closest',
            margin=dict(b=20, l=5, r=5, t=60),
            annotations=[
                dict(
                    text=f"N√≥s azuis: Disserta√ß√µes | N√≥s azul escuro: Teses<br>"
                         f"Conex√µes indicam similaridade > {threshold}<br>"
                         f"Tamanho do n√≥ = n√∫mero de conex√µes",
                    showarrow=False,
                    xref="paper", yref="paper",
                    x=0.005, y=-0.002,
                    xanchor='left', yanchor='bottom',
                    font=dict(size=10, color='#666')
                )
            ],
            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
            plot_bgcolor='white',
            paper_bgcolor='white'
        )
    )
    
    return fig, G

# Inicializa√ß√£o do NLTK
@st.cache_resource
def init_nltk():
    """Inicializa recursos do NLTK"""
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    
    try:
        nltk.data.find('corpora/stopwords')
    except LookupError:
        nltk.download('stopwords')
    
    try:
        nltk.data.find('corpora/rslp')
    except LookupError:
        nltk.download('rslp')

# Fun√ß√£o principal
def main():
    # Inicializar NLTK
    init_nltk()
    
    # T√≠tulo principal
    st.markdown('<h1 class="header-title">üìö Acervo PPGDR v1</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; color: #666; font-size: 1.2rem;">Plataforma de Consulta do Acervo de Teses e Disserta√ß√µes do PPGDR-FURB</p>', unsafe_allow_html=True)
    
    # Carregar dados
    df = load_data()
    embeddings = load_embeddings()
    client = init_openai_client()
    
    if df.empty:
        st.warning("Nenhum dado foi carregado. Por favor, verifique se o arquivo CSV est√° dispon√≠vel.")
        return
    
    # Preparar dados
    if 'Assuntos_Processados' not in df.columns:
        df['Assuntos_Processados'] = df['Assuntos_Lista'].apply(safe_literal_eval)
    
    # Status dos embeddings e API
    if embeddings is not None:
        st.sidebar.markdown(f'<span class="embedding-badge">‚úÖ Embeddings Ativos</span>', unsafe_allow_html=True)
        st.sidebar.markdown(f"Dimens√£o: {embeddings.shape}")
    else:
        st.sidebar.markdown("‚ö†Ô∏è Embeddings n√£o dispon√≠veis")
    
    if client is not None:
        st.sidebar.markdown(f'<span class="api-badge">‚úÖ API OpenAI Ativa</span>', unsafe_allow_html=True)
    else:
        st.sidebar.markdown("‚ö†Ô∏è API OpenAI n√£o configurada")
    
    # Sidebar para navega√ß√£o
    st.sidebar.markdown('<h2 style="color: #0F5EDD;">üîç Navega√ß√£o</h2>', unsafe_allow_html=True)
    
    # Op√ß√µes de navega√ß√£o
    opcoes = ["üè† P√°gina Principal", "üìä Dashboard", "üï∏Ô∏è Rede de Similaridade"]
    escolha = st.sidebar.selectbox("Selecione uma op√ß√£o:", opcoes)
    
    if escolha == "üè† P√°gina Principal":
        pagina_principal(df, embeddings, client)
    elif escolha == "üìä Dashboard":
        dashboard(df)
    elif escolha == "üï∏Ô∏è Rede de Similaridade":
        pagina_rede_similaridade(df, embeddings, client)

def pagina_principal(df, embeddings, client):
    """P√°gina principal com busca e visualiza√ß√£o dos dados"""
    
    # M√©tricas gerais
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #0F5EDD; margin: 0;">Total de Trabalhos</h3>
            <h2 style="margin: 0;">{}</h2>
        </div>
        """.format(len(df)), unsafe_allow_html=True)
    
    with col2:
        dissertacoes = len(df[df['Tipo_Documento'] == 'Disserta√ß√£o'])
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #0F5EDD; margin: 0;">Disserta√ß√µes</h3>
            <h2 style="margin: 0;">{}</h2>
        </div>
        """.format(dissertacoes), unsafe_allow_html=True)
    
    with col3:
        teses = len(df[df['Tipo_Documento'] == 'Tese'])
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #0F5EDD; margin: 0;">Teses</h3>
            <h2 style="margin: 0;">{}</h2>
        </div>
        """.format(teses), unsafe_allow_html=True)
    
    with col4:
        anos = df['Ano'].nunique()
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #0F5EDD; margin: 0;">Anos de Produ√ß√£o</h3>
            <h2 style="margin: 0;">{}</h2>
        </div>
        """.format(anos), unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Se√ß√£o de busca
    st.markdown('<h2 class="section-header">üîç Busca no Acervo</h2>', unsafe_allow_html=True)
    
    # Abas para diferentes tipos de busca
    if embeddings is not None and client is not None:
        tab1, tab2, tab3, tab4 = st.tabs([
            "üìã Busca por Assunto", 
            "üî§ Busca por Palavras-chave", 
            "üß† Busca Sem√¢ntica (TF-IDF)", 
            "üöÄ Busca Sem√¢ntica OpenAI"
        ])
        
        with tab4:
            busca_semantica_openai(df, embeddings, client)
    elif embeddings is not None:
        tab1, tab2, tab3, tab4 = st.tabs([
            "üìã Busca por Assunto", 
            "üî§ Busca por Palavras-chave", 
            "üß† Busca Sem√¢ntica (TF-IDF)", 
            "üöÄ Busca Sem√¢ntica Avan√ßada"
        ])
        
        with tab4:
            busca_semantica_avancada(df, embeddings)
    else:
        tab1, tab2, tab3 = st.tabs([
            "üìã Busca por Assunto", 
            "üî§ Busca por Palavras-chave", 
            "üß† Busca Sem√¢ntica"
        ])
    
    with tab1:
        busca_por_assunto(df)
    
    with tab2:
        busca_por_palavras_chave(df)
    
    with tab3:
        busca_semantica(df)
    
    st.markdown("---")
    
    # Exibi√ß√£o do dataframe completo
    st.markdown('<h2 class="section-header">üìä Dados Gerais do Acervo</h2>', unsafe_allow_html=True)
    
    # Filtros para o dataframe
    col1, col2, col3 = st.columns(3)
    
    with col1:
        tipos_selecionados = st.multiselect(
            "Filtrar por Tipo de Documento:",
            options=df['Tipo_Documento'].unique(),
            default=df['Tipo_Documento'].unique()
        )
    
    with col2:
        anos_selecionados = st.multiselect(
            "Filtrar por Ano:",
            options=sorted(df['Ano'].unique()),
            default=sorted(df['Ano'].unique())
        )
    
    with col3:
        orientadores_selecionados = st.multiselect(
            "Filtrar por Orientador:",
            options=sorted(df['Orientador'].unique()) if 'Orientador' in df.columns else [],
            default=sorted(df['Orientador'].unique()) if 'Orientador' in df.columns else []
        )
    
    # Aplicar filtros
    df_filtrado = df[
        (df['Tipo_Documento'].isin(tipos_selecionados)) &
        (df['Ano'].isin(anos_selecionados))
    ]
    
    if 'Orientador' in df.columns and orientadores_selecionados:
        df_filtrado = df_filtrado[df_filtrado['Orientador'].isin(orientadores_selecionados)]
    
    # Exibir dataframe filtrado
    st.dataframe(
        df_filtrado[['Autor', 'T√≠tulo', 'Ano', 'Tipo_Documento', 'Orientador'] if 'Orientador' in df.columns else ['Autor', 'T√≠tulo', 'Ano', 'Tipo_Documento']],
        use_container_width=True,
        height=400
    )
    
    # Detalhes expand√≠veis para cada registro
    if not df_filtrado.empty:
        st.markdown('<h3 class="section-header">üìñ Detalhes dos Trabalhos</h3>', unsafe_allow_html=True)
        
        for idx, row in df_filtrado.iterrows():
            with st.expander(f"+ Detalhes: {row['T√≠tulo'][:100]}..."):
                exibir_detalhes_trabalho(row, df, embeddings)

def busca_por_assunto(df):
    """Implementa a busca por assunto usando dropdown"""
    
    # Gerar lista de assuntos √∫nicos e ordenados
    todos_assuntos = [assunto for sublista in df['Assuntos_Processados'] for assunto in sublista]
    lista_unica = list(set(todos_assuntos))
    lista_ordenada = sorted(lista_unica, key=lambda texto: remover_acentos(texto.lower()))
    
    # Dropdown para sele√ß√£o de assunto
    assunto_selecionado = st.selectbox(
        "Selecione um assunto:",
        options=['-- Selecione um Assunto --'] + lista_ordenada,
        key="busca_assunto"
    )
    
    if assunto_selecionado != '-- Selecione um Assunto --':
        # Filtrar dataframe pelo assunto selecionado
        filtro_booleano = df['Assuntos_Processados'].apply(lambda lista: assunto_selecionado in lista)
        df_filtrado = df[filtro_booleano]
        
        if not df_filtrado.empty:
            st.success(f"Encontrados {len(df_filtrado)} trabalho(s) para o assunto: '{assunto_selecionado}'")
            
            # Exibir resultados
            for idx, row in df_filtrado.iterrows():
                with st.expander(f"üìÑ {row['T√≠tulo']}"):
                    exibir_detalhes_trabalho(row, df)
        else:
            st.warning("Nenhum trabalho encontrado para o assunto selecionado.")

def busca_por_palavras_chave(df):
    """Implementa a busca por palavras-chave"""
    
    # Campo de entrada para palavras-chave
    palavras_chave = st.text_input(
        "Digite palavras-chave para busca:",
        placeholder="Ex: desenvolvimento regional, sustentabilidade, economia",
        key="busca_palavras"
    )
    
    if palavras_chave:
        # Buscar nos t√≠tulos e resumos
        palavras_normalizadas = normalizar_string(palavras_chave)
        
        # Filtrar por t√≠tulo
        filtro_titulo = df['T√≠tulo'].apply(
            lambda x: any(palavra in normalizar_string(str(x)) for palavra in palavras_normalizadas.split())
        )
        
        # Filtrar por resumo LLM
        filtro_resumo = pd.Series([False] * len(df))
        if 'Resumo_LLM' in df.columns:
            filtro_resumo = df['Resumo_LLM'].apply(
                lambda x: any(palavra in normalizar_string(str(x)) for palavra in palavras_normalizadas.split())
            )
        
        # Combinar filtros
        df_filtrado = df[filtro_titulo | filtro_resumo]
        
        if not df_filtrado.empty:
            st.success(f"Encontrados {len(df_filtrado)} trabalho(s) para as palavras-chave: '{palavras_chave}'")
            
            # Exibir resultados
            for idx, row in df_filtrado.iterrows():
                with st.expander(f"üìÑ {row['T√≠tulo']}"):
                    exibir_detalhes_trabalho(row, df)
        else:
            st.warning("Nenhum trabalho encontrado para as palavras-chave especificadas.")

@st.cache_resource
def preparar_busca_semantica(df):
    """Prepara os dados para busca sem√¢ntica"""
    
    # Combinar t√≠tulo e resumo LLM para busca sem√¢ntica
    textos_combinados = []
    for idx, row in df.iterrows():
        texto = str(row['T√≠tulo'])
        if 'Resumo_LLM' in df.columns and pd.notna(row['Resumo_LLM']):
            texto += " " + str(row['Resumo_LLM'])
        textos_combinados.append(texto)
    
    # Preprocessar textos
    textos_processados = [preprocessar_texto(texto) for texto in textos_combinados]
    
    # Criar matriz TF-IDF
    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    tfidf_matrix = vectorizer.fit_transform(textos_processados)
    
    return vectorizer, tfidf_matrix, textos_processados

def busca_semantica(df):
    """Implementa a busca sem√¢ntica usando TF-IDF e similaridade de cosseno"""
    
    # Campo de entrada para busca sem√¢ntica
    query_semantica = st.text_input(
        "Digite sua consulta para busca sem√¢ntica:",
        placeholder="Ex: impactos ambientais do desenvolvimento urbano",
        key="busca_semantica"
    )
    
    if query_semantica:
        try:
            # Preparar dados para busca sem√¢ntica
            vectorizer, tfidf_matrix, textos_processados = preparar_busca_semantica(df)
            
            # Realizar busca
            resultados = buscar_e_rankear(query_semantica, False, vectorizer, tfidf_matrix)
            
            if resultados:
                st.success(f"Encontrados {len(resultados)} trabalho(s) relevantes para: '{query_semantica}'")
                
                # Exibir resultados ordenados por relev√¢ncia
                for i, (idx, score) in enumerate(resultados[:10]):  # Top 10 resultados
                    row = df.iloc[idx]
                    with st.expander(f"üìÑ {row['T√≠tulo']} (Relev√¢ncia: {score:.3f})"):
                        exibir_detalhes_trabalho(row, df)
            else:
                st.warning("Nenhum trabalho encontrado para a consulta especificada.")
                
        except Exception as e:
            st.error(f"Erro na busca sem√¢ntica: {str(e)}")

def busca_semantica_avancada(df, embeddings):
    """Implementa a busca sem√¢ntica usando embeddings pr√©-calculados (sem API)"""
    
    st.info("üöÄ **Busca Sem√¢ntica Avan√ßada** - Usando embeddings pr√©-calculados")
    st.warning("‚ö†Ô∏è **Limita√ß√£o**: Esta vers√£o simula a busca sem√¢ntica pois n√£o h√° API OpenAI configurada.")
    
    # Campo de entrada para busca sem√¢ntica avan√ßada
    query_avancada = st.text_input(
        "Digite sua consulta para busca sem√¢ntica avan√ßada:",
        placeholder="Ex: desenvolvimento sustent√°vel na regi√£o sul do Brasil",
        key="busca_semantica_avancada"
    )
    
    if query_avancada:
        try:
            # Usar TF-IDF como simula√ß√£o
            vectorizer, tfidf_matrix, _ = preparar_busca_semantica(df)
            resultados = buscar_e_rankear(query_avancada, False, vectorizer, tfidf_matrix)
            
            if resultados:
                st.success(f"Encontrados {len(resultados)} trabalho(s) relevantes para: '{query_avancada}'")
                
                # Exibir resultados ordenados por relev√¢ncia
                for i, (idx, score) in enumerate(resultados[:10]):
                    row = df.iloc[idx]
                    
                    # Criar card com score destacado
                    col1, col2 = st.columns([4, 1])
                    
                    with col1:
                        st.markdown(f"**{i+1}. {row['T√≠tulo']}**")
                        st.markdown(f"*{row['Autor']} ({row['Ano']})*")
                    
                    with col2:
                        st.markdown(f'<span class="similarity-score">Similaridade: {score:.3f}</span>', 
                                  unsafe_allow_html=True)
                    
                    # Resumo formatado
                    if 'Resumo_LLM' in df.columns and pd.notna(row['Resumo_LLM']):
                        resumo_formatado = textwrap.fill(str(row['Resumo_LLM']), width=100)
                        st.markdown(f"**Resumo:** {resumo_formatado}")
                    
                    st.markdown("---")
                    
            else:
                st.warning("Nenhum trabalho encontrado para a consulta especificada.")
                
        except Exception as e:
            st.error(f"Erro na busca sem√¢ntica avan√ßada: {str(e)}")

def busca_semantica_openai(df, embeddings, client):
    """Implementa a busca sem√¢ntica real usando API OpenAI"""
    
    st.info("üöÄ **Busca Sem√¢ntica OpenAI** - Usando API OpenAI para gerar embeddings da query em tempo real")
    
    # Campo de entrada para busca sem√¢ntica OpenAI
    query_openai = st.text_input(
        "Digite sua consulta para busca sem√¢ntica OpenAI:",
        placeholder="Ex: desenvolvimento sustent√°vel na regi√£o sul do Brasil",
        key="busca_semantica_openai"
    )
    
    if query_openai:
        with st.spinner("Gerando embedding da query e calculando similaridades..."):
            try:
                # Realizar busca sem√¢ntica real com API OpenAI
                resultados = buscar_semantica_openai_real(query_openai, client, embeddings)
                
                if resultados:
                    st.success(f"Encontrados {len(resultados)} trabalho(s) altamente relevantes para: '{query_openai}'")
                    
                    # Exibir resultados ordenados por relev√¢ncia
                    for i, (idx, score) in enumerate(resultados[:10]):
                        row = df.iloc[idx]
                        
                        # Criar card com score destacado
                        col1, col2 = st.columns([4, 1])
                        
                        with col1:
                            st.markdown(f"**{i+1}. {row['T√≠tulo']}**")
                            st.markdown(f"*{row['Autor']} ({row['Ano']})*")
                        
                        with col2:
                            st.markdown(f'<span class="similarity-score">Similaridade: {score:.3f}</span>', 
                                      unsafe_allow_html=True)
                        
                        # Resumo formatado
                        if 'Resumo_LLM' in df.columns and pd.notna(row['Resumo_LLM']):
                            resumo_formatado = textwrap.fill(str(row['Resumo_LLM']), width=100)
                            st.markdown(f"**Resumo:** {resumo_formatado}")
                        
                        st.markdown("---")
                        
                else:
                    st.warning("Nenhum trabalho encontrado para a consulta especificada.")
                    
            except Exception as e:
                st.error(f"Erro na busca sem√¢ntica OpenAI: {str(e)}")

def pagina_rede_similaridade(df, embeddings, client):
    """P√°gina da rede de similaridade com sumariza√ß√£o tem√°tica"""
    
    st.markdown('<h1 class="header-title">üï∏Ô∏è Rede de Similaridade</h1>', unsafe_allow_html=True)
    st.markdown("Visualiza√ß√£o interativa das conex√µes entre trabalhos baseada em similaridade sem√¢ntica.")
    
    # Controles da rede
    col1, col2, col3 = st.columns(3)
    
    with col1:
        threshold = st.slider(
            "Threshold de Similaridade:",
            min_value=0.1,
            max_value=0.8,
            value=0.3,
            step=0.05,
            help="Conex√µes s√≥ aparecem se a similaridade for maior que este valor"
        )
    
    with col2:
        max_nodes = st.slider(
            "M√°ximo de Trabalhos:",
            min_value=10,
            max_value=50,
            value=20,
            step=5,
            help="N√∫mero m√°ximo de trabalhos a exibir na rede"
        )
    
    with col3:
        filtro_tipo = st.selectbox(
            "Filtrar por Tipo:",
            options=["Todos", "Disserta√ß√£o", "Tese"],
            help="Filtrar trabalhos por tipo de documento"
        )
    
    # Aplicar filtro de tipo se necess√°rio
    df_filtrado = df.copy()
    if filtro_tipo != "Todos":
        df_filtrado = df_filtrado[df_filtrado['Tipo_Documento'] == filtro_tipo]
    
    if len(df_filtrado) < 2:
        st.warning("N√£o h√° trabalhos suficientes para criar uma rede de similaridade.")
        return
    
    # Gerar e exibir a rede
    with st.spinner("Gerando rede de similaridade..."):
        try:
            fig, grafo = criar_rede_similaridade(df_filtrado, embeddings, threshold, max_nodes)
            
            if fig is not None and grafo is not None:
                st.plotly_chart(fig, use_container_width=True, height=600)
                
                # Informa√ß√µes adicionais
                st.markdown("### üìä Informa√ß√µes da Rede")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Trabalhos Exibidos", min(max_nodes, len(df_filtrado)))
                
                with col2:
                    # Calcular n√∫mero de conex√µes
                    conexoes = len(grafo.edges())
                    st.metric("Conex√µes", conexoes)
                
                with col3:
                    num_nodes = len(grafo.nodes())
                    densidade = (2 * conexoes) / (num_nodes * (num_nodes - 1)) if num_nodes > 1 else 0
                    st.metric("Densidade da Rede", f"{densidade:.2%}")
                
                # Sumariza√ß√£o Tem√°tica
                if client is not None and len(grafo.nodes()) > 0:
                    st.markdown("### ü§ñ Sumariza√ß√£o Tem√°tica do Cluster")
                    
                    if st.button("üöÄ Gerar Sum√°rio Tem√°tico do Cluster", type="primary"):
                        with st.spinner("Gerando sum√°rio tem√°tico com OpenAI..."):
                            try:
                                # Preparar documentos do cluster
                                documentos_cluster = []
                                for node in grafo.nodes():
                                    node_data = grafo.nodes[node]
                                    idx_original = node_data['idx_original']
                                    row = df_filtrado.iloc[idx_original]
                                    documentos_cluster.append({
                                        'T√≠tulo': row['T√≠tulo'],
                                        'Resumo_LLM': row['Resumo_LLM'] if 'Resumo_LLM' in row else ''
                                    })
                                
                                # Gerar sum√°rio
                                sumario = gerar_sumario_cluster(documentos_cluster, client)
                                
                                # Exibir sum√°rio
                                st.markdown(f"""
                                <div class="summary-box">
                                    <h4 style="color: #0F5EDD; margin-top: 0;">üìã Sum√°rio Tem√°tico do Cluster</h4>
                                    {sumario}
                                </div>
                                """, unsafe_allow_html=True)
                                
                                # Informa√ß√µes do cluster
                                st.markdown(f"""
                                <div class="cluster-info">
                                    <strong>Informa√ß√µes do Cluster:</strong><br>
                                    ‚Ä¢ N√∫mero de trabalhos analisados: {len(documentos_cluster)}<br>
                                    ‚Ä¢ Threshold de similaridade: {threshold}<br>
                                    ‚Ä¢ Conex√µes na rede: {conexoes}<br>
                                    ‚Ä¢ Modelo usado: GPT-4o-mini
                                </div>
                                """, unsafe_allow_html=True)
                                
                            except Exception as e:
                                st.error(f"Erro ao gerar sum√°rio tem√°tico: {str(e)}")
                else:
                    if client is None:
                        st.info("üîß **Sumariza√ß√£o Tem√°tica**: Configure a API OpenAI para habilitar esta funcionalidade.")
                    else:
                        st.info("üìä Gere uma rede de similaridade para usar a sumariza√ß√£o tem√°tica.")
                
                # Explica√ß√£o
                st.markdown("""
                ### üí° Como Interpretar a Rede
                
                - **N√≥s (c√≠rculos)**: Cada n√≥ representa um trabalho (tese ou disserta√ß√£o)
                - **Cores**: Azul claro = Disserta√ß√µes, Azul escuro = Teses  
                - **Conex√µes (linhas)**: Indicam alta similaridade sem√¢ntica entre trabalhos
                - **Tamanho dos n√≥s**: Proporcional ao n√∫mero de conex√µes (centralidade)
                - **Posicionamento**: Trabalhos similares tendem a ficar pr√≥ximos
                - **Clusters**: Grupos de trabalhos conectados indicam temas relacionados
                
                ### ü§ñ Sumariza√ß√£o Tem√°tica
                
                - **Funcionalidade**: Analisa todos os trabalhos da rede e gera um resumo tem√°tico
                - **Tecnologia**: Usa GPT-4o-mini para identificar temas centrais e termos comuns
                - **Resultado**: S√≠ntese dos principais temas e palavras-chave do cluster
                
                **Dica**: Passe o mouse sobre os n√≥s para ver detalhes dos trabalhos!
                """)
            
        except Exception as e:
            st.error(f"Erro ao gerar rede de similaridade: {str(e)}")

def exibir_detalhes_trabalho(row, df_completo, embeddings=None):
    """Exibe os detalhes de um trabalho espec√≠fico"""
    
    # Informa√ß√µes b√°sicas
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown(f"**Autor:** {row['Autor']}")
        st.markdown(f"**Ano:** {row['Ano']}")
        st.markdown(f"**Tipo:** {row['Tipo_Documento']}")
        if 'Orientador' in row:
            st.markdown(f"**Orientador:** {row['Orientador']}")
    
    with col2:
        if 'Assuntos_Lista' in row:
            st.markdown("**Assuntos:**")
            assuntos = safe_literal_eval(row['Assuntos_Lista'])
            for assunto in assuntos[:5]:  # Mostrar apenas os primeiros 5
                st.markdown(f"‚Ä¢ {assunto}")
    
    # Resumo LLM
    if 'Resumo_LLM' in row and pd.notna(row['Resumo_LLM']):
        st.markdown("**Resumo:**")
        st.markdown(f'<div class="expandable-content">{row["Resumo_LLM"]}</div>', unsafe_allow_html=True)
    
    # Trabalhos similares
    if embeddings is not None:
        st.markdown("**Trabalhos Similares:**")
        try:
            # Encontrar √≠ndice do trabalho atual
            idx_atual = df_completo.index[df_completo['T√≠tulo'] == row['T√≠tulo']].tolist()
            if idx_atual:
                idx_atual = idx_atual[0]
                
                # Calcular similaridade com todos os outros trabalhos
                matriz_sim = calcular_matriz_similaridade(df_completo, embeddings)
                similaridades = matriz_sim[idx_atual]
                
                # Encontrar os 3 mais similares (excluindo o pr√≥prio trabalho)
                indices_similares = np.argsort(-similaridades)[1:4]  # Exclui o primeiro (pr√≥prio trabalho)
                
                for i, idx_similar in enumerate(indices_similares):
                    if similaridades[idx_similar] > 0.3:  # Threshold m√≠nimo
                        trabalho_similar = df_completo.iloc[idx_similar]
                        score = similaridades[idx_similar]
                        st.markdown(f"‚Ä¢ **{trabalho_similar['T√≠tulo']}** (Similaridade: {score:.3f})")
                        st.markdown(f"  *{trabalho_similar['Autor']} ({trabalho_similar['Ano']})*")
        except Exception as e:
            st.info(f"An√°lise de similaridade n√£o dispon√≠vel: {str(e)}")
    else:
        st.markdown("**Rede de Similaridade:**")
        st.info("Funcionalidade de rede de similaridade estar√° dispon√≠vel quando os embeddings forem carregados.")

def dashboard(df):
    """P√°gina do dashboard com visualiza√ß√µes"""
    
    st.markdown('<h1 class="header-title">üìä Dashboard do Acervo</h1>', unsafe_allow_html=True)
    
    # Gr√°fico 1: Quantidade de trabalhos por tipo
    st.markdown('<h2 class="section-header">üìà Distribui√ß√£o por Tipo de Documento</h2>', unsafe_allow_html=True)
    
    contagem_docs = df['Tipo_Documento'].value_counts().reset_index()
    contagem_docs.columns = ['Tipo_Documento', 'Quantidade']
    
    fig1 = px.bar(
        contagem_docs,
        x='Tipo_Documento',
        y='Quantidade',
        title='Quantidade de Trabalhos por Tipo de Documento',
        text='Quantidade',
        color='Tipo_Documento',
        color_discrete_map={
            'Disserta√ß√£o': '#0F5EDD',
            'Tese': '#0A4BC7'
        }
    )
    
    fig1.update_traces(textposition='outside', textfont_size=12)
    fig1.update_layout(
        showlegend=False,
        title_x=0.5,
        font=dict(family="Arial, sans-serif", size=12)
    )
    
    st.plotly_chart(fig1, use_container_width=True)
    
    # Gr√°fico 2: Top 20 assuntos mais frequentes
    st.markdown('<h2 class="section-header">üè∑Ô∏è Assuntos Mais Frequentes</h2>', unsafe_allow_html=True)
    
    todos_assuntos = [assunto for sublista in df['Assuntos_Processados'] for assunto in sublista]
    contador_assuntos = Counter(todos_assuntos)
    top_20_assuntos = contador_assuntos.most_common(20)
    
    df_top20 = pd.DataFrame(top_20_assuntos, columns=['Assunto', 'Quantidade'])
    
    fig2 = px.bar(
        df_top20.sort_values(by='Quantidade', ascending=True),
        x='Quantidade',
        y='Assunto',
        orientation='h',
        title='Top 20 Assuntos Mais Frequentes nos Trabalhos',
        text='Quantidade'
    )
    
    fig2.update_traces(marker_color='#0F5EDD', textposition='outside')
    fig2.update_layout(
        yaxis=dict(tickmode='linear'),
        xaxis_title="N√∫mero de Ocorr√™ncias",
        yaxis_title=None,
        margin=dict(l=150, r=20, t=50, b=50),
        title_x=0.5
    )
    
    st.plotly_chart(fig2, use_container_width=True)
    
    # Gr√°fico 3: Produ√ß√£o anual
    st.markdown('<h2 class="section-header">üìÖ Produ√ß√£o Anual</h2>', unsafe_allow_html=True)
    
    contagem_agrupada = df.groupby(['Ano', 'Tipo_Documento']).size().reset_index(name='Quantidade')
    contagem_agrupada = contagem_agrupada.sort_values('Ano')
    
    fig3 = px.bar(
        contagem_agrupada,
        x='Ano',
        y='Quantidade',
        color='Tipo_Documento',
        title='Produ√ß√£o Anual: Teses vs. Disserta√ß√µes',
        barmode='group',
        color_discrete_map={
            'Disserta√ß√£o': '#0F5EDD',
            'Tese': '#0A4BC7'
        }
    )
    
    fig3.update_layout(
        title_x=0.5,
        font=dict(family="Arial, sans-serif", size=12),
        legend_title_text='Tipo de Documento'
    )
    
    fig3.update_xaxes(type='category')
    
    st.plotly_chart(fig3, use_container_width=True)
    
    # Estat√≠sticas adicionais
    st.markdown('<h2 class="section-header">üìä Estat√≠sticas Gerais</h2>', unsafe_allow_html=True)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #0F5EDD; margin: 0;">Per√≠odo de Produ√ß√£o</h3>
            <h2 style="margin: 0;">{} - {}</h2>
        </div>
        """.format(df['Ano'].min(), df['Ano'].max()), unsafe_allow_html=True)
    
    with col2:
        media_anual = len(df) / (df['Ano'].max() - df['Ano'].min() + 1)
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #0F5EDD; margin: 0;">M√©dia Anual</h3>
            <h2 style="margin: 0;">{:.1f}</h2>
        </div>
        """.format(media_anual), unsafe_allow_html=True)
    
    with col3:
        total_assuntos = len(set([assunto for sublista in df['Assuntos_Processados'] for assunto in sublista]))
        st.markdown("""
        <div class="metric-card">
            <h3 style="color: #0F5EDD; margin: 0;">Assuntos √önicos</h3>
            <h2 style="margin: 0;">{}</h2>
        </div>
        """.format(total_assuntos), unsafe_allow_html=True)

if __name__ == "__main__":
    main()

